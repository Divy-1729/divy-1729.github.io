<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention Is All We Need</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            }
        };
    </script>
</head>
<body>
    <header class="header">
        <nav class="nav">
            <div class="nav-brand">
                <a href="../index.html">Home</a>
            </div>
            <div class="nav-links">
                <a href="../index.html" class="nav-link">Home</a>
                <a href="../blog.html" class="nav-link">Blog</a>
                <a href="../about.html" class="nav-link">About</a>
                <a href="https://github.com/Divy-1729" class="nav-link" target="_blank" rel="noopener noreferrer">
                    GitHub ↗
                </a>
            </div>
        </nav>
    </header>

    <main class="main">
        <div class="container">
            <a href="../blog.html" class="back-link">Back to blog</a>
            <article class="blog-post">
                <div class="post-date">January 2025</div>
                <h1 class="post-title">Attention Is All We Need</h1>
                <div class="post-content">

                    <p>
                        I finally decided to sit down and write a blog post on the transformer paper by Google,
                        <em>Attention Is All You Need</em>. My goal is to help you, the reader, understand this paper
                        and this architecture with as little prerequisite knowledge as possible.
                    </p>

                    <p>
                        At the highest level, you can think of the transformer as a sequence to sequence
                        translation model.
                    </p>

                    <p>
                        <code>I like cats</code> → transformer → <code>j’aime les chats</code>
                    </p>

                    <p>
                        We can understand everything about the transformer by following the journey of these
                        words.
                    </p>

                    <p>
                        From the architecture diagram, we see some distinct features. Here is what the pipeline
                        looks like:
                    </p>

                    <p>
                        <code>
                            input → embedding → encoder <br>
                            output → decoder (with a link from the encoder) → linear layer + softmax → prediction
                        </code>
                    </p>

                    <!-- Architecture image (put transformer-architecture.jpg in the same folder as this HTML file) -->
                    <div class="post-image">
                        <img src="transformer-architecture.jpg" alt="Transformer architecture diagram from Attention Is All You Need">
                        <p class="image-caption">
                            The Transformer architecture as introduced in Attention Is All You Need.
                        </p>
                    </div>

                    <p>
                        Let’s first start by understanding how a transformer makes predictions. A transformer is
                        autoregressive. This means it uses all of its previous predictions to make the current
                        prediction.
                    </p>

                    <p>
                        So if you feed the transformer the sentence <code>I like cats</code>, here is what the
                        process looks like when the transformer generates a translation:
                    </p>

                    <p>
                        <code>I like cats</code> → transformer → <code>&lt;BOS&gt;</code>
                        (a beginning of sentence token that tells the decoder to start generating)
                    </p>

                    <p>
                        <code>I like cats &lt;BOS&gt;</code> → transformer → <code>J’</code>
                    </p>

                    <p>
                        <code>I like cats &lt;BOS&gt; J’</code> → transformer → <code>aime</code>
                    </p>

                    <p>
                        <code>I like cats &lt;BOS&gt; J’ aime</code> → transformer → <code>les</code>
                    </p>

                    <p>
                        And we continue this process until the model predicts <code>&lt;EOS&gt;</code>, the end of
                        sentence token.
                    </p>

                    <h2>What is a token?</h2>

                    <p>
                        If you've understood this next, the next question you might be thinking about is exactly what a token is.
                        In simple terms, tokens are just parts of a sentence, like words. Most modern LLMs today use a tokenizer
                        to convert tokens to words. This is a really fucked up but cool algorithm called Byte Pair Encoding.
                    </p>

                    <!-- Tokenization image (put tokens_to_word.jpg in the same folder as this HTML file) -->
                    <div class="post-image">
                        <img src="tokens_to_words.jpg" alt="Example of tokenization into subword pieces (Byte Pair Encoding)">
                        <p class="image-caption">
                            Example: breaking a sentence into tokens (often subword pieces).
                        </p>
                    </div>

                    <p>
                        As you see in the image, we successfully tokenized our sentence into sub-parts.
                        The next step is to convert this token into an embedding.
                    </p>

                    <!-- Embedding image (put embedding.jpg in the same folder as this HTML file) -->
                    <div class="post-image">
                        <img src="embedding.jpg" alt="Tokens converted into vectors (embeddings)">
                        <p class="image-caption">
                            Tokens are mapped into vectors (embeddings) so the model can work with numbers.
                        </p>
                    </div>

                    <p>
                        An embedding is simply a high-dimensional representation of our word. Transformers are based on an
                        architecture called neural networks, and these networks can unfortunately only understand and work with
                        matrices full of numbers. The embedding is just the transformation of this sentence into a matrix full of numbers.
                    </p>

                    <p>
                        There are many known algorithms to create embeddings, some of which are word2vec and GloVe.
                        These are very interesting papers I recommend reading. A classic example people use to show that embeddings
                        capture meaning is:
                    </p>

                    <p>
                        $$
                        \text{vector}(\text{king}) - \text{vector}(\text{man}) + \text{vector}(\text{woman}) \approx \text{vector}(\text{queen})
                        $$
                    </p>

                    <h2>Positional encoding</h2>

                    <p>
                        After creating the embedding, the transformer introduces a positional encoding → essentially a means to encode the position
                        of a token in a sentence.
                    </p>

                    <p>
                        In the original transformer paper, positional encodings are added to embeddings and are defined using sine and cosine:
                    </p>

                    <p>
                        $$
                        PE(pos, 2i) = \sin\Big(\frac{pos}{10000^{2i/d_{\text{model}}}}\Big)
                        $$
                        $$
                        PE(pos, 2i+1) = \cos\Big(\frac{pos}{10000^{2i/d_{\text{model}}}}\Big)
                        $$
                    </p>

                    <p>
                        Here, $pos$ is the position of the token in the sentence (0, 1, 2, ...), $i$ is the dimension index, and $d_{\text{model}}$
                        is the embedding size.
                    </p>

                </div>
            </article>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-links">
                <a href="../index.html" class="footer-link">Home</a>
                <a href="../blog.html" class="footer-link">Blog</a>
                <a href="../about.html" class="footer-link">About</a>
                <a href="https://github.com/Divy-1729" class="footer-link" target="_blank" rel="noopener noreferrer">GitHub</a>
            </div>
            <div class="footer-copyright">
                © 2025
            </div>
        </div>
    </footer>
</body>
</html>
