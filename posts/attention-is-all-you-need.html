<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Attention Is All We Need</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            }
        };
    </script>
</head>
<body>
    <header class="header">
        <nav class="nav">
            <div class="nav-brand">
                <a href="../index.html">Home</a>
            </div>
            <div class="nav-links">
                <a href="../index.html" class="nav-link">Home</a>
                <a href="../blog.html" class="nav-link">Blog</a>
                <a href="../about.html" class="nav-link">About</a>
                <a href="https://github.com/Divy-1729" class="nav-link" target="_blank" rel="noopener noreferrer">
                    GitHub ↗
                </a>
            </div>
        </nav>
    </header>

    <main class="main">
        <div class="container">
            <a href="../blog.html" class="back-link">Back to blog</a>
            <article class="blog-post">
                <div class="post-date">January 2025</div>
                <h1 class="post-title">Attention Is All We Need</h1>
                <div class="post-content">

                    <p>
                        I finally decided to sit down and write a blog post on the transformer paper by Google,
                        <em>Attention Is All You Need</em>. My goal is to help you, the reader, understand this paper
                        and this architecture with as little prerequisite knowledge as possible.
                    </p>

                    <p>
                        At the highest level, you can think of the transformer as a sequence to sequence
                        translation model.
                    </p>

                    <p>
                        <code>I like cats</code> → transformer → <code>j’aime les chats</code>
                    </p>

                    <p>
                        We can understand everything about the transformer by following the journey of these
                        words.
                    </p>

                    <p>
                        From the architecture diagram, we see some distinct features. Here is what the pipeline
                        looks like:
                    </p>

                    <p>
                        <code>
                            input → embedding → encoder <br>
                            output → decoder (with a link from the encoder) → linear layer + softmax → prediction
                        </code>
                    </p>

                    <!-- Architecture image (put transformer-architecture.jpg in the same folder as this HTML file) -->
                    <div class="post-image">
                        <img src="transformer-architecture.jpg" alt="Transformer architecture diagram from Attention Is All You Need">
                        <p class="image-caption">
                            The Transformer architecture as introduced in Attention Is All You Need.
                        </p>
                    </div>

                    <p>
                        Let’s first start by understanding how a transformer makes predictions. A transformer is
                        autoregressive. This means it uses all of its previous predictions to make the current
                        prediction.
                    </p>

                    <p>
                        So if you feed the transformer the sentence <code>I like cats</code>, here is what the
                        process looks like when the transformer generates a translation:
                    </p>

                    <p>
                        <code>I like cats</code> → transformer → <code>&lt;BOS&gt;</code>
                        (a beginning of sentence token that tells the decoder to start generating)
                    </p>

                    <p>
                        <code>I like cats &lt;BOS&gt;</code> → transformer → <code>J’</code>
                    </p>

                    <p>
                        <code>I like cats &lt;BOS&gt; J’</code> → transformer → <code>aime</code>
                    </p>

                    <p>
                        <code>I like cats &lt;BOS&gt; J’ aime</code> → transformer → <code>les</code>
                    </p>

                    <p>
                        And we continue this process until the model predicts <code>&lt;EOS&gt;</code>, the end of
                        sentence token.
                    </p>

                    <h2>What is a token?</h2>

                    <p>
                        If you've understood this next, the next question you might be thinking about is exactly what a token is.
                        In simple terms, tokens are just parts of a sentence, like words. Most modern LLMs today use a tokenizer
                        to convert tokens to words. This is a really fucked up but cool algorithm called Byte Pair Encoding.
                    </p>

                    <!-- Tokenization image (put tokens_to_words.jpg in the same folder as this HTML file) -->
                    <div class="post-image">
                        <img src="tokens_to_words.jpg" alt="Example of tokenization into subword pieces (Byte Pair Encoding)">
                        <p class="image-caption">
                            Example: breaking a sentence into tokens (often subword pieces).
                        </p>
                    </div>

                    <p>
                        As you see in the image, we successfully tokenized our sentence into sub-parts.
                        The next step is to convert this token into an embedding.
                    </p>

                    <!-- Embedding image (put embedding.jpg in the same folder as this HTML file) -->
                    <div class="post-image">
                        <img src="embedding.jpg" alt="Tokens converted into vectors (embeddings)">
                        <p class="image-caption">
                            Tokens are mapped into vectors (embeddings) so the model can work with numbers.
                        </p>
                    </div>

                    <p>
                        An embedding is simply a high-dimensional representation of our word. Transformers are based on an
                        architecture called neural networks, and these networks can unfortunately only understand and work with
                        matrices full of numbers. The embedding is just the transformation of this sentence into a matrix full of numbers.
                    </p>

                    <p>
                        There are many known algorithms to create embeddings, some of which are word2vec and GloVe.
                        These are very interesting papers I recommend reading. A classic example people use to show that embeddings
                        capture meaning is:
                    </p>

                    <p>
                        $$
                        \text{vector}(\text{king}) - \text{vector}(\text{man}) + \text{vector}(\text{woman}) \approx \text{vector}(\text{queen})
                        $$
                    </p>

                    <h2>Positional encoding</h2>

                    <p>
                        After creating the embedding, the transformer introduces a positional encoding → essentially a means to encode the position
                        of a token in a sentence.
                    </p>

                    <p>
                        In the original transformer paper, positional encodings are added to embeddings and are defined using sine and cosine:
                    </p>

                    <p>
                        $$
                        PE(pos, 2i) = \sin\Big(\frac{pos}{10000^{2i/d_{\text{model}}}}\Big)
                        $$
                        $$
                        PE(pos, 2i+1) = \cos\Big(\frac{pos}{10000^{2i/d_{\text{model}}}}\Big)
                        $$
                    </p>

                    <p>
                        Here, $pos$ is the position of the token in the sentence (0, 1, 2, ...), $i$ is the dimension index, and $d_{\text{model}}$
                        is the embedding size.
                    </p>

                    <h2>The encoder</h2>

                    <p>
                        Now that we have converted our input text into an embedding, it is time to encode our input embedding.
                        Here is what the encoder looks like:
                    </p>

                    <!-- Encoder image (put encoder.png in the same folder as this HTML file) -->
                    <div class="post-image">
                        <img src="encoder.png" alt="Transformer encoder block diagram">
                        <p class="image-caption">
                            The Transformer encoder block.
                        </p>
                    </div>

                    <p>
                        Let’s go into each part of the encoder in detail, but before that, we need to define three key terms:
                        Query, Key, Value.
                    </p>

                    <h3>Query, Key, Value (search engine intuition)</h3>

                    <p>
                        A useful way to think about attention is like a search engine.
                        You type a search query, the search engine looks through a database of pages, and it returns the most relevant ones.
                    </p>

                    <ul>
                        <li><strong>Query (Q)</strong>: what you are looking for. For a given token, it represents the question that token is asking about the rest of the sentence.</li>
                        <li><strong>Key (K)</strong>: what each token offers as a label or summary. Keys are what the model matches the query against.</li>
                        <li><strong>Value (V)</strong>: the actual information you want to retrieve once you find a match. Values are the content that gets mixed together.</li>
                    </ul>

                    <p>
                        Luckily for us, all of these vectors for query, key, and value are essentially derived from the same input embedding.
                        Concretely, we take the embedding matrix $X \in \mathbb{R}^{n \times d_{\text{model}}}$ and multiply it by three learned weight matrices:
                    </p>

                    <p>
                        $$
                        Q = XW^Q,\quad K = XW^K,\quad V = XW^V
                        $$
                    </p>

                    <p>
                        Each row corresponds to a token, so $n$ is the number of tokens in the sentence.
                        The dimensions are usually:
                        $Q \in \mathbb{R}^{n \times d_k}$, $K \in \mathbb{R}^{n \times d_k}$, $V \in \mathbb{R}^{n \times d_v}$.
                    </p>

                    <h2>Scaled dot product attention</h2>

                    <p>
                        The most important contribution of the transformer is multi-head attention.
                        To understand it, we must first define attention, also called scaled dot product attention.
                    </p>

                    <p>
                        Scaled dot product attention defines how much "attention" the transformer pays to all the other input tokens in a sentence,
                        given one token. It is computed by:
                    </p>

                    <p>
                        $$
                        \text{Attention}(Q, K, V) = \text{softmax}\Big(\frac{QK^\top}{\sqrt{d_k}}\Big)V
                        $$
                    </p>

                    <p>
                        Note the normalization term $\sqrt{d_k}$. We divide by $\sqrt{d_k}$ because as the dimension grows,
                        dot products tend to get larger in magnitude, which can make the softmax extremely peaky.
                        That can lead to tiny gradients and unstable training. Scaling keeps the numbers in a nicer range.
                    </p>

                    <p>
                        As you might have noticed, attention has an $O(n^2)$ time complexity because $QK^\top$ is an $n \times n$ matrix.
                        This is one of the biggest problems in transformers, and many workarounds have been introduced, like FlashAttention.
                    </p>

                    <p>
                        This is a nice visualization of attention I found from Jay Alammar’s blog:
                    </p>

                    <!-- Attention visualization image (put attention.png in the same folder as this HTML file) -->
                    <div class="post-image">
                        <img src="attention.png" alt="Attention visualization">
                        <p class="image-caption">
                            A visualization of attention weights.
                        </p>
                    </div>

                    <h2>Self-attention, masked self-attention, and cross-attention</h2>

                    <p>
                        Attention in the encoder is called self-attention because the query, key, and value vectors all come from the encoder input.
                    </p>

                    <p>
                        A couple things are different in the decoder. One is <strong>masked self-attention</strong>.
                        The decoder generates tokens one at a time, and when it is predicting the next token, it is not allowed to "peek" at future tokens.
                        So we apply a mask that forces attention to only look left, at earlier positions in the output sequence.
                        This is what makes the decoder truly autoregressive.
                    </p>

                    <p>
                        The decoder also does <strong>cross-attention</strong>, where the keys and values come from the encoder output,
                        while the queries come from the decoder. Intuitively, the decoder is asking:
                        "Given what I have generated so far, which parts of the input sentence should I focus on right now?"
                    </p>

                </div>
            </article>
        </div>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-links">
                <a href="../index.html" class="footer-link">Home</a>
                <a href="../blog.html" class="footer-link">Blog</a>
                <a href="../about.html" class="footer-link">About</a>
                <a href="https://github.com/Divy-1729" class="footer-link">About</a>
                <a href="https://github.com/Divy-1729" class="footer-link" target="_blank" rel="noopener noreferrer">GitHub</a>
            </div>
            <div class="footer-copyright">
                © 2025
            </div>
        </div>
    </footer>
</body>
</html>
